{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxaq3lhjt_wy",
        "outputId": "b7647b38-0d9e-4121-902b-1ccdf1f7c865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Requirement already satisfied: flash_attn in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash_attn) (2.1.0+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash_attn) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash_attn) (23.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash_attn) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash_attn) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash_attn) (1.3.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement transtormers (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transtormers\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.37.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.26.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "# !huggingface-cli login \n",
        "!pip install datasets > -\n",
        "!pip install rouge_score > -\n",
        "!pip install flash_attn\n",
        "!pip install transtormers\n",
        "!pip install --upgrade transformers\n",
        "!pip install accelerate\n",
        "!pip install peft\n",
        "!pip install bitsandbytes\n",
        "!pip install -qqq trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX8BQ7Z4hxHj"
      },
      "source": [
        "#stablelm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwwcEU1uCSgl",
        "outputId": "f1044e69-dc6a-4286-9937-1f08b8587ae3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "StableLMEpochForCausalLM(\n",
              "  (model): StableLMEpochModel(\n",
              "    (embed_tokens): Embedding(50304, 2560)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x DecoderLayer(\n",
              "        (self_attn): Attention(\n",
              "          (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
              "          (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
              "          (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
              "          (o_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
              "          (rotary_emb): RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): MLP(\n",
              "          (gate_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
              "          (up_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
              "          (down_proj): Linear(in_features=6912, out_features=2560, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2560, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-3b-4e1t\", trust_remote_code=True,torch_dtype=\"auto\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"stabilityai/stablelm-3b-4e1t\", trust_remote_code=True,torch_dtype=\"auto\")\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRNsIOf-7PLi"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "AIC0qdcpjiRf",
        "outputId": "f43a1fcb-aa13-4ea2-e150-04114ae701ee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-33401f46-dfbc-4861-a1c9-6932021a57bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetSumm--train--1</th>\n",
              "      <th>TweetSumm--train--2</th>\n",
              "      <th>TweetSumm--train--3</th>\n",
              "      <th>TweetSumm--train--4</th>\n",
              "      <th>TweetSumm--train--5</th>\n",
              "      <th>TweetSumm--train--6</th>\n",
              "      <th>TweetSumm--train--7</th>\n",
              "      <th>TweetSumm--train--8</th>\n",
              "      <th>TweetSumm--train--9</th>\n",
              "      <th>TweetSumm--train--10</th>\n",
              "      <th>...</th>\n",
              "      <th>TweetSumm--train--870</th>\n",
              "      <th>TweetSumm--train--871</th>\n",
              "      <th>TweetSumm--train--872</th>\n",
              "      <th>TweetSumm--train--873</th>\n",
              "      <th>TweetSumm--train--874</th>\n",
              "      <th>TweetSumm--train--875</th>\n",
              "      <th>TweetSumm--train--876</th>\n",
              "      <th>TweetSumm--train--877</th>\n",
              "      <th>TweetSumm--train--878</th>\n",
              "      <th>TweetSumm--train--879</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>original dialog id</th>\n",
              "      <td>b065262210783596c1fe79466b8f8985</td>\n",
              "      <td>1e1d8fd4f95c984fb78687c9e946dc97</td>\n",
              "      <td>b5773077fa55c381260390472deff4c2</td>\n",
              "      <td>3574d6b4418cb546a90d1561bacd66a2</td>\n",
              "      <td>f30d2bbc15e7ff32244761b38b67d160</td>\n",
              "      <td>20327b84e8b6c205b7ad7e56b2861c66</td>\n",
              "      <td>78dcc694d8c4a59a870ef61b746994f7</td>\n",
              "      <td>6fc68bfd953819cf6ec1329b4e6831ac</td>\n",
              "      <td>01a14bf0dff4f25a0ee571acf08d779e</td>\n",
              "      <td>d63f8327d930c484d1a1e5b2c919315c</td>\n",
              "      <td>...</td>\n",
              "      <td>3b490c0aaf5a696c2a58c5aa6fe5bf24</td>\n",
              "      <td>d3836c129bacc591c25a45765acbc0b5</td>\n",
              "      <td>37132101337b20b4f69a0aa13c05459d</td>\n",
              "      <td>c472e3f71cc64ecdcd6f08ce8c9e2755</td>\n",
              "      <td>f5e6ca64b1772c3e92335768d212cf16</td>\n",
              "      <td>b13be8337844e91117624554916697e4</td>\n",
              "      <td>e621b7b5441a040536ea9b5bfabba1ae</td>\n",
              "      <td>8541230f7864cb62eb815aaca20630d6</td>\n",
              "      <td>eb4a1c2507cd9a2801c3bad17b6ab49a</td>\n",
              "      <td>da32c7eb1693234417600b1bd3d3dcfc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dialog index</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>870</td>\n",
              "      <td>871</td>\n",
              "      <td>872</td>\n",
              "      <td>873</td>\n",
              "      <td>874</td>\n",
              "      <td>875</td>\n",
              "      <td>876</td>\n",
              "      <td>877</td>\n",
              "      <td>878</td>\n",
              "      <td>879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>original dialog info</th>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [], 'ab...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "      <td>{'summaries': {'extractive_summaries': [[{'is_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>log</th>\n",
              "      <td>[{'turn id': 1, 'user utterance': 'So neither ...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@115850 hi ...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@AskAmex Wh...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@AmazonHelp...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@GWRHelp I'...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@115802 @Ai...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': 'I don't kno...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@USCellular...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@XboxSuppor...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@AmazonHelp...</td>\n",
              "      <td>...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@XboxSuppor...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@129913 Hi,...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': 'not my faul...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@AppleSuppo...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@MicrosoftH...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '', 'system ...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@Tesco Yet ...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@O2 Your PA...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': '@NortonSupp...</td>\n",
              "      <td>[{'turn id': 1, 'user utterance': 'I'm done! #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prompt</th>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 879 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33401f46-dfbc-4861-a1c9-6932021a57bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33401f46-dfbc-4861-a1c9-6932021a57bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33401f46-dfbc-4861-a1c9-6932021a57bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8dd8301a-b757-4695-b1ff-6e10610aefb1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8dd8301a-b757-4695-b1ff-6e10610aefb1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8dd8301a-b757-4695-b1ff-6e10610aefb1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                    TweetSumm--train--1  \\\n",
              "original dialog id                     b065262210783596c1fe79466b8f8985   \n",
              "dialog index                                                          1   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': 'So neither ...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                    TweetSumm--train--2  \\\n",
              "original dialog id                     1e1d8fd4f95c984fb78687c9e946dc97   \n",
              "dialog index                                                          2   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@115850 hi ...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                    TweetSumm--train--3  \\\n",
              "original dialog id                     b5773077fa55c381260390472deff4c2   \n",
              "dialog index                                                          3   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@AskAmex Wh...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                    TweetSumm--train--4  \\\n",
              "original dialog id                     3574d6b4418cb546a90d1561bacd66a2   \n",
              "dialog index                                                          4   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@AmazonHelp...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                    TweetSumm--train--5  \\\n",
              "original dialog id                     f30d2bbc15e7ff32244761b38b67d160   \n",
              "dialog index                                                          5   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@GWRHelp I'...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                    TweetSumm--train--6  \\\n",
              "original dialog id                     20327b84e8b6c205b7ad7e56b2861c66   \n",
              "dialog index                                                          6   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@115802 @Ai...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                    TweetSumm--train--7  \\\n",
              "original dialog id                     78dcc694d8c4a59a870ef61b746994f7   \n",
              "dialog index                                                          7   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': 'I don't kno...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                    TweetSumm--train--8  \\\n",
              "original dialog id                     6fc68bfd953819cf6ec1329b4e6831ac   \n",
              "dialog index                                                          8   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@USCellular...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                    TweetSumm--train--9  \\\n",
              "original dialog id                     01a14bf0dff4f25a0ee571acf08d779e   \n",
              "dialog index                                                          9   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@XboxSuppor...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                   TweetSumm--train--10  ...  \\\n",
              "original dialog id                     d63f8327d930c484d1a1e5b2c919315c  ...   \n",
              "dialog index                                                         10  ...   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...  ...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@AmazonHelp...  ...   \n",
              "prompt                                                               []  ...   \n",
              "\n",
              "                                                  TweetSumm--train--870  \\\n",
              "original dialog id                     3b490c0aaf5a696c2a58c5aa6fe5bf24   \n",
              "dialog index                                                        870   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@XboxSuppor...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                  TweetSumm--train--871  \\\n",
              "original dialog id                     d3836c129bacc591c25a45765acbc0b5   \n",
              "dialog index                                                        871   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@129913 Hi,...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                  TweetSumm--train--872  \\\n",
              "original dialog id                     37132101337b20b4f69a0aa13c05459d   \n",
              "dialog index                                                        872   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': 'not my faul...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                  TweetSumm--train--873  \\\n",
              "original dialog id                     c472e3f71cc64ecdcd6f08ce8c9e2755   \n",
              "dialog index                                                        873   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@AppleSuppo...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                  TweetSumm--train--874  \\\n",
              "original dialog id                     f5e6ca64b1772c3e92335768d212cf16   \n",
              "dialog index                                                        874   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@MicrosoftH...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                  TweetSumm--train--875  \\\n",
              "original dialog id                     b13be8337844e91117624554916697e4   \n",
              "dialog index                                                        875   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [], 'ab...   \n",
              "log                   [{'turn id': 1, 'user utterance': '', 'system ...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                  TweetSumm--train--876  \\\n",
              "original dialog id                     e621b7b5441a040536ea9b5bfabba1ae   \n",
              "dialog index                                                        876   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@Tesco Yet ...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                  TweetSumm--train--877  \\\n",
              "original dialog id                     8541230f7864cb62eb815aaca20630d6   \n",
              "dialog index                                                        877   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@O2 Your PA...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                  TweetSumm--train--878  \\\n",
              "original dialog id                     eb4a1c2507cd9a2801c3bad17b6ab49a   \n",
              "dialog index                                                        878   \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...   \n",
              "log                   [{'turn id': 1, 'user utterance': '@NortonSupp...   \n",
              "prompt                                                               []   \n",
              "\n",
              "                                                  TweetSumm--train--879  \n",
              "original dialog id                     da32c7eb1693234417600b1bd3d3dcfc  \n",
              "dialog index                                                        879  \n",
              "original dialog info  {'summaries': {'extractive_summaries': [[{'is_...  \n",
              "log                   [{'turn id': 1, 'user utterance': 'I'm done! #...  \n",
              "prompt                                                               []  \n",
              "\n",
              "[5 rows x 879 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "# Specify the file path\n",
        "json_file_path = \"/content/drive/MyDrive/kaggle/train.json\"\n",
        "df_train = pd.read_json(json_file_path)\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32YjWPE6_6mK",
        "outputId": "a256faea-4a52-44c1-dd8e-d41a8dc657c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1429: FutureWarning: The repository for Salesforce/dialogstudio contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Salesforce/dialogstudio\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "dataset = load_dataset(\"Salesforce/dialogstudio\", \"TweetSumm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sV_5SYb8sUFf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def generate_prompt(conversation: str) -> str:\n",
        "    return f\"\"\"### Instruction:\\nBelow is a conversation between a human and an agent. Write a summary of the conversation.\n",
        "\n",
        "### Input:\n",
        "{conversation.strip()}\n",
        "\"\"\".strip()\n",
        "\n",
        "def cleaning(text):\n",
        "\n",
        "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "    return re.sub(r\"\\^[^ ]+\", \"\", text)\n",
        "\n",
        "\n",
        "def conversation_extraction(data_point):\n",
        "    text = \"\"\n",
        "    for item in data_point[\"log\"]:\n",
        "        customer = cleaning(item[\"user utterance\"])\n",
        "        text += f\"Customer: {customer.strip()}\\n\"\n",
        "\n",
        "        agent = cleaning(item[\"system response\"])\n",
        "        text += f\"Agent: {agent.strip()}\\n\"\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def redesign_data(data_point):\n",
        "    summaries = json.loads(data_point[\"original dialog info\"])[\"summaries\"][\n",
        "        \"abstractive_summaries\"\n",
        "    ]\n",
        "    summary = summaries[0]\n",
        "    summary = \" \".join(summary)\n",
        "\n",
        "    conversation_text = conversation_extraction(data_point)\n",
        "    text = \"### Instruction:\\nBelow is a conversation between a human and an agent. Write a summary of the conversation.\\n\\n### Input:\\n\" + conversation_text +  \"\\n\\n### Output:\\n\" + summary\n",
        "\n",
        "    return {\n",
        "        \"conversation\": conversation_text,\n",
        "        \"summary\": summary,\n",
        "        \"text\": text,\n",
        "        \"prompt\": generate_prompt(conversation_text),\n",
        "    }\n",
        "\n",
        "\n",
        "def preprocess_dataset(data: Dataset):\n",
        "    return (\n",
        "        data.shuffle(seed=42)\n",
        "        .map(redesign_data)\n",
        "        .remove_columns(\n",
        "            [\n",
        "                \"original dialog id\",\"new dialog id\",\"dialog index\",\n",
        "                \"original dialog info\",\"log\",\"prompt\",\n",
        "            ]\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wnyXakqQ9jq3"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"] = preprocess_dataset(dataset[\"train\"])\n",
        "dataset[\"test\"] = preprocess_dataset(dataset[\"test\"])\n",
        "dataset[\"validation\"] = preprocess_dataset(dataset[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0O7SdCrDfmp",
        "outputId": "44a49302-b918-4036-ef26-1bfa345e4153"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-f3aa68319492>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge = load_metric('rouge')\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:752: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "from tqdm import tqdm\n",
        "rouge = load_metric('rouge')\n",
        "example_indices = [109]\n",
        "line = \"---------------------------------------------------------------------------------------------------\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dir3nbGBN4w9"
      },
      "source": [
        "#part1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhusfGc3AAgS"
      },
      "source": [
        "zero-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcbAjW99_0EJ",
        "outputId": "2e581ff3-93a3-41f6-8a86-29f64b86a277"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:752: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example  109\n",
            "---------------------------------------------------------------------------------------------------\n",
            "prompt for zero-shot:\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
            "Customer: if you have a commercial that says you have 100mbs for 44.99 honor it in all your markets you provide 100mbs at.\n",
            "Agent: Good morning. In some areas of our markets our network is not yet able to provide those speeds. We do not offer a...\n",
            "Customer: My area offered me 100mb for more money so now that this is the standard and they do offer 100mb shouldnâ€™t I receive it right away\n",
            "Agent: I would be happy to take a look. If you upgraded to the 100 plan and are not receiving this we can investigate. H...\n",
            "Customer: I have the 60mb plan but I watch many commercials saying they offer 100mb for the same price. My area has 100mb so shouldnâ€™t I be getting it\n",
            "Agent: I am sorry, the advertisements are package/product prices for new customers. If you would like to upgrade that sh...\n",
            "Customer: So you leave existing customers below your new standards is what your telling me, right?\n",
            "Agent: Hi William, In order to discuss this further we ask that you please contact our billing specialist at: 800-892-4357â€‹â€‹. Thank you.\n",
            "\n",
            "Summary:\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "human summary:\n",
            "Customer having an issue with data speed in his area. Agent updated the customer in some area of their market network is not yet provided those speed and also informed the customer to contact billing specialist for further assist.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "model output for zero-shot:\n",
            "\n",
            "The customer is upset that he is not receiving the advertised speeds. The agent explains that the customer is not receiving the advertised speeds because the customer is not a new customer. The agent then asks the customer to contact the billing department.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "{'rouge1': 0.2682926829268293, 'rouge2': 0.125, 'rougeL': 0.2682926829268293, 'rougeLsum': 0.2682926829268293}\n"
          ]
        }
      ],
      "source": [
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "model.config.eos_token_id = tokenizer.eos_token_id\n",
        "results = []\n",
        "for index in example_indices:\n",
        "    conversation = dataset['test'][index]['conversation']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "    prompt = f\"\"\"Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
        "{conversation}\n",
        "Summary:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "    token = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=47,\n",
        "        temperature=0.9\n",
        "        )\n",
        "    completion_tokens = token[0][inputs['input_ids'].size(1):]\n",
        "    completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
        "    print('Example ', index)\n",
        "    print(line)\n",
        "    print(f'prompt for zero-shot:\\n{line}\\n{prompt}')\n",
        "    print(line)\n",
        "    print(f'human summary:\\n{summary}')\n",
        "    print(line)\n",
        "    print(f'model output for zero-shot:\\n{completion}\\n')\n",
        "    print(line)\n",
        "    results.append(rouge.compute(predictions=[completion], references=[summary]))\n",
        "    average_score = {\n",
        "    metric: sum([result[metric].mid.fmeasure for result in results]) / len(results)\n",
        "    for metric in results[0]}\n",
        "    print(average_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kmO075H3YGk"
      },
      "source": [
        "average of rouge for all training dataset by zero-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFZRMdNm3EPu",
        "outputId": "d73a11c2-1c5e-410d-804f-090008097e3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/110 [00:03<06:39,  3.66s/it]\u001b[A\n",
            "  2%|â–         | 2/110 [00:06<05:12,  2.90s/it]\u001b[A\n",
            "  3%|â–Ž         | 3/110 [00:08<04:32,  2.55s/it]\u001b[A\n",
            "  4%|â–Ž         | 4/110 [00:10<04:23,  2.49s/it]\u001b[A\n",
            "  5%|â–         | 5/110 [00:12<04:14,  2.42s/it]\u001b[A\n",
            "  5%|â–Œ         | 6/110 [00:15<04:21,  2.51s/it]\u001b[A\n",
            "  6%|â–‹         | 7/110 [00:18<04:24,  2.56s/it]\u001b[A\n",
            "  7%|â–‹         | 8/110 [00:20<04:04,  2.40s/it]\u001b[A\n",
            "  8%|â–Š         | 9/110 [00:23<04:19,  2.57s/it]\u001b[A\n",
            "  9%|â–‰         | 10/110 [00:26<04:52,  2.93s/it]\u001b[A\n",
            " 10%|â–ˆ         | 11/110 [00:30<05:15,  3.19s/it]\u001b[A\n",
            " 11%|â–ˆ         | 12/110 [00:35<06:14,  3.82s/it]\u001b[A\n",
            " 12%|â–ˆâ–        | 13/110 [00:38<05:21,  3.31s/it]\u001b[A\n",
            " 13%|â–ˆâ–Ž        | 14/110 [00:40<04:41,  2.93s/it]\u001b[A\n",
            " 14%|â–ˆâ–Ž        | 15/110 [00:42<04:23,  2.77s/it]\u001b[A\n",
            " 15%|â–ˆâ–        | 16/110 [00:45<04:32,  2.90s/it]\u001b[A\n",
            " 15%|â–ˆâ–Œ        | 17/110 [00:52<06:10,  3.98s/it]\u001b[A\n",
            " 16%|â–ˆâ–‹        | 18/110 [00:56<06:06,  3.99s/it]\u001b[A\n",
            " 17%|â–ˆâ–‹        | 19/110 [00:59<05:43,  3.77s/it]\u001b[A\n",
            " 18%|â–ˆâ–Š        | 20/110 [01:02<05:22,  3.58s/it]\u001b[A\n",
            " 19%|â–ˆâ–‰        | 21/110 [01:06<05:14,  3.54s/it]\u001b[A\n",
            " 20%|â–ˆâ–ˆ        | 22/110 [01:08<04:36,  3.15s/it]\u001b[A\n",
            " 21%|â–ˆâ–ˆ        | 23/110 [01:11<04:31,  3.13s/it]\u001b[A\n",
            " 22%|â–ˆâ–ˆâ–       | 24/110 [01:14<04:29,  3.13s/it]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–Ž       | 25/110 [01:19<05:18,  3.75s/it]\u001b[A\n",
            " 24%|â–ˆâ–ˆâ–Ž       | 26/110 [01:23<05:20,  3.81s/it]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–       | 27/110 [01:26<04:47,  3.46s/it]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–Œ       | 28/110 [01:28<04:10,  3.06s/it]\u001b[A\n",
            " 26%|â–ˆâ–ˆâ–‹       | 29/110 [01:30<03:53,  2.88s/it]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–‹       | 30/110 [01:34<04:06,  3.08s/it]\u001b[A\n",
            " 28%|â–ˆâ–ˆâ–Š       | 31/110 [01:39<04:41,  3.56s/it]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–‰       | 32/110 [01:41<04:09,  3.20s/it]\u001b[A\n",
            " 30%|â–ˆâ–ˆâ–ˆ       | 33/110 [01:43<03:42,  2.88s/it]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆ       | 34/110 [01:46<03:37,  2.86s/it]\u001b[A\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–      | 35/110 [01:51<04:28,  3.58s/it]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 36/110 [01:57<05:02,  4.09s/it]\u001b[A\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 37/110 [02:00<04:47,  3.94s/it]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–      | 38/110 [02:04<04:44,  3.96s/it]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 39/110 [02:10<05:19,  4.50s/it]\u001b[A\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 40/110 [02:13<04:51,  4.16s/it]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 41/110 [02:17<04:40,  4.06s/it]\u001b[A\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 42/110 [02:20<04:18,  3.80s/it]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 43/110 [02:25<04:26,  3.97s/it]\u001b[A\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 44/110 [02:29<04:22,  3.97s/it]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 45/110 [02:32<03:57,  3.65s/it]\u001b[A\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/110 [02:35<03:44,  3.51s/it]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 47/110 [02:39<03:53,  3.71s/it]\u001b[A\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 48/110 [02:43<03:58,  3.84s/it]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 49/110 [02:47<03:48,  3.75s/it]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 50/110 [02:50<03:35,  3.59s/it]\u001b[A\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 51/110 [02:55<04:00,  4.08s/it]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 52/110 [02:59<03:51,  3.99s/it]\u001b[A\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 53/110 [03:02<03:30,  3.70s/it]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 54/110 [03:05<03:15,  3.49s/it]\u001b[A\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 55/110 [03:07<02:57,  3.23s/it]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 56/110 [03:12<03:18,  3.67s/it]\u001b[A\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 57/110 [03:16<03:12,  3.63s/it]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 58/110 [03:20<03:23,  3.91s/it]\u001b[A\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 59/110 [03:24<03:23,  3.98s/it]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 60/110 [03:29<03:29,  4.20s/it]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 61/110 [03:32<03:08,  3.85s/it]\u001b[A\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 62/110 [03:36<03:07,  3.90s/it]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 63/110 [03:40<03:05,  3.94s/it]\u001b[A\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/110 [03:44<03:06,  4.06s/it]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 65/110 [03:48<02:55,  3.91s/it]\u001b[A\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 66/110 [03:51<02:39,  3.63s/it]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 67/110 [03:54<02:29,  3.48s/it]\u001b[A\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 68/110 [03:58<02:32,  3.64s/it]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 69/110 [04:03<02:41,  3.95s/it]\u001b[A\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 70/110 [04:06<02:28,  3.71s/it]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 71/110 [04:09<02:18,  3.55s/it]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 72/110 [04:13<02:14,  3.54s/it]\u001b[A\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 73/110 [04:15<01:59,  3.24s/it]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 74/110 [04:17<01:37,  2.71s/it]\u001b[A\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 75/110 [04:19<01:29,  2.56s/it]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 76/110 [04:21<01:25,  2.52s/it]\u001b[A\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 77/110 [04:24<01:24,  2.55s/it]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 78/110 [04:26<01:17,  2.41s/it]\u001b[A\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 79/110 [04:30<01:32,  2.99s/it]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 80/110 [04:34<01:38,  3.28s/it]\u001b[A\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 81/110 [04:38<01:39,  3.44s/it]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 82/110 [04:41<01:33,  3.34s/it]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 83/110 [04:44<01:27,  3.26s/it]\u001b[A\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 84/110 [04:47<01:19,  3.07s/it]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 85/110 [04:50<01:15,  3.01s/it]\u001b[A\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 86/110 [04:52<01:09,  2.91s/it]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 87/110 [04:55<01:05,  2.85s/it]\u001b[A\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 88/110 [04:59<01:07,  3.05s/it]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 89/110 [05:01<01:00,  2.90s/it]\u001b[A\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 90/110 [05:03<00:53,  2.69s/it]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 91/110 [05:06<00:49,  2.59s/it]\u001b[A\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 92/110 [05:08<00:46,  2.60s/it]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 93/110 [05:11<00:43,  2.54s/it]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 94/110 [05:14<00:43,  2.71s/it]\u001b[A\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 95/110 [05:16<00:39,  2.65s/it]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 96/110 [05:19<00:36,  2.60s/it]\u001b[A\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 97/110 [05:21<00:33,  2.56s/it]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 98/110 [05:24<00:30,  2.55s/it]\u001b[A\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 99/110 [05:27<00:29,  2.71s/it]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 100/110 [05:30<00:27,  2.72s/it]\u001b[A\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 101/110 [05:32<00:24,  2.72s/it]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 102/110 [05:35<00:20,  2.55s/it]\u001b[A\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 103/110 [05:37<00:17,  2.48s/it]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 104/110 [05:39<00:13,  2.25s/it]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 105/110 [05:41<00:11,  2.39s/it]\u001b[A\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 106/110 [05:44<00:09,  2.47s/it]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 107/110 [05:46<00:07,  2.38s/it]\u001b[A\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 108/110 [05:48<00:04,  2.34s/it]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 109/110 [05:51<00:02,  2.33s/it]\u001b[A\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [05:53<00:00,  3.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge1': 0.28147042730716154, 'rouge2': 0.08427375021738195, 'rougeL': 0.23239024097682343, 'rougeLsum': 0.22743986580540443}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "for index in tqdm(range(110)):\n",
        "    conversation = dataset['test'][index]['conversation']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "    prompt = f\"\"\"Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
        "{conversation}\n",
        "Summary:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "    token = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=47,\n",
        "        temperature=0.9\n",
        "        )\n",
        "    completion_tokens = token[0][inputs['input_ids'].size(1):]\n",
        "    completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
        "    results.append(rouge.compute(predictions=[completion], references=[summary]))\n",
        "average_score = {metric: sum([result[metric].mid.fmeasure for result in results]) / len(results)\n",
        "                  for metric in results[0]}\n",
        "print(average_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHnQ2ldXQXB5"
      },
      "source": [
        "one-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B6jd54hQd7f",
        "outputId": "3314cded-9b5f-4231-97b0-9605d2bad2b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is a conversation between a human and an AI agent. With a summary of the conversation.\n",
            "Customer: On went to buy a Lip na mpesa wrist band in your shop in Thika and was sent to Annas Mall where i dint find your Team!\n",
            "Agent: Apologies for this, check with any M-PESA Agent, or Safaricom authorized dealer. You may also go back to our shop in Thika -(contd)to verify whether we have restocked.\n",
            "Customer: I was at your shop at Thika and they told me they were not sure where the guys dealing with the wrist bands were\n",
            "Agent: The mpesa 1 tap is not available at Thika its being piloted in Nairobi, Mombasa, Kisumu, Eldoret and Nyeri towns ... cont... currently. To get it you may visit the nearest town which is Nairobi to purchase one at a cost of 20/-.\n",
            "Customer: Would you please confirm for me if I will get the wrist band at your shop in Moi Avenue today so I don't have to travel for nothing.\n",
            "Agent: Hi, yes you will and at the moment it can only be used in the towns mentioned in our previous response only.\n",
            "Customer: On Tuesday I went to your shop on Moi Avenue to get wrist band but was told they ended,but was told I would be informed when they are ready Cont...Are they still not ready yet? ?\n",
            "Agent: Hi, you can visit any authorized dealer or the other shops in town for one.\n",
            "\n",
            "Summary:\n",
            "Customer wants the wrist band which was not available in the stop so customer says to confirm if the wrist band is present. Agent says to visit any authorized dealer or shop in town for one.\n",
            "\n",
            "Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
            "Customer: if you have a commercial that says you have 100mbs for 44.99 honor it in all your markets you provide 100mbs at.\n",
            "Agent: Good morning. In some areas of our markets our network is not yet able to provide those speeds. We do not offer a...\n",
            "Customer: My area offered me 100mb for more money so now that this is the standard and they do offer 100mb shouldnâ€™t I receive it right away\n",
            "Agent: I would be happy to take a look. If you upgraded to the 100 plan and are not receiving this we can investigate. H...\n",
            "Customer: I have the 60mb plan but I watch many commercials saying they offer 100mb for the same price. My area has 100mb so shouldnâ€™t I be getting it\n",
            "Agent: I am sorry, the advertisements are package/product prices for new customers. If you would like to upgrade that sh...\n",
            "Customer: So you leave existing customers below your new standards is what your telling me, right?\n",
            "Agent: Hi William, In order to discuss this further we ask that you please contact our billing specialist at: 800-892-4357â€‹â€‹. Thank you.\n",
            "\n",
            "Summary:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def make_prompt(examples, sample):\n",
        "    prompt = ''\n",
        "    for index in examples:\n",
        "        conversation = dataset['test'][index]['conversation']\n",
        "        summary = dataset['test'][index]['summary']\n",
        "        prompt += f\"\"\"Below is a conversation between a human and an AI agent. With a summary of the conversation.\n",
        "{conversation}\n",
        "Summary:\n",
        "{summary}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    conversation = dataset['test'][sample]['conversation']\n",
        "\n",
        "    prompt += f\"\"\"Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
        "{conversation}\n",
        "Summary:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "prompt = print(make_prompt([30], 109))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHigxrT0CurM",
        "outputId": "f5a2e4d5-a298-45f0-a47a-ef25f00ee09c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example  109\n",
            "---------------------------------------------------------------------------------------------------\n",
            "prompt for one-shot:\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Below is a conversation between a human and an AI agent. With a summary of the conversation.\n",
            "Customer: On went to buy a Lip na mpesa wrist band in your shop in Thika and was sent to Annas Mall where i dint find your Team!\n",
            "Agent: Apologies for this, check with any M-PESA Agent, or Safaricom authorized dealer. You may also go back to our shop in Thika -(contd)to verify whether we have restocked.\n",
            "Customer: I was at your shop at Thika and they told me they were not sure where the guys dealing with the wrist bands were\n",
            "Agent: The mpesa 1 tap is not available at Thika its being piloted in Nairobi, Mombasa, Kisumu, Eldoret and Nyeri towns ... cont... currently. To get it you may visit the nearest town which is Nairobi to purchase one at a cost of 20/-.\n",
            "Customer: Would you please confirm for me if I will get the wrist band at your shop in Moi Avenue today so I don't have to travel for nothing.\n",
            "Agent: Hi, yes you will and at the moment it can only be used in the towns mentioned in our previous response only.\n",
            "Customer: On Tuesday I went to your shop on Moi Avenue to get wrist band but was told they ended,but was told I would be informed when they are ready Cont...Are they still not ready yet? ?\n",
            "Agent: Hi, you can visit any authorized dealer or the other shops in town for one.\n",
            "\n",
            "Summary:\n",
            "Customer wants the wrist band which was not available in the stop so customer says to confirm if the wrist band is present. Agent says to visit any authorized dealer or shop in town for one.\n",
            "\n",
            "Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
            "Customer: if you have a commercial that says you have 100mbs for 44.99 honor it in all your markets you provide 100mbs at.\n",
            "Agent: Good morning. In some areas of our markets our network is not yet able to provide those speeds. We do not offer a...\n",
            "Customer: My area offered me 100mb for more money so now that this is the standard and they do offer 100mb shouldnâ€™t I receive it right away\n",
            "Agent: I would be happy to take a look. If you upgraded to the 100 plan and are not receiving this we can investigate. H...\n",
            "Customer: I have the 60mb plan but I watch many commercials saying they offer 100mb for the same price. My area has 100mb so shouldnâ€™t I be getting it\n",
            "Agent: I am sorry, the advertisements are package/product prices for new customers. If you would like to upgrade that sh...\n",
            "Customer: So you leave existing customers below your new standards is what your telling me, right?\n",
            "Agent: Hi William, In order to discuss this further we ask that you please contact our billing specialist at: 800-892-4357â€‹â€‹. Thank you.\n",
            "\n",
            "Summary:\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "human summary:\n",
            "Customer having an issue with data speed in his area. Agent updated the customer in some area of their market network is not yet provided those speed and also informed the customer to contact billing specialist for further assist.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "model output for one-shot:\n",
            "Customer wants to know if the 100mb is available in his area. Agent says that the 100mb is not available in the area. Customer says that the area should be able to get the 100mb. Agent says that the area is not able to\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "{'rouge1': 0.3132530120481928, 'rouge2': 0.09876543209876543, 'rougeL': 0.26506024096385544, 'rougeLsum': 0.26506024096385544}\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "for index in example_indices:\n",
        "    conversation = dataset['test'][index]['conversation']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    prompt = make_prompt([30], 109)\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "    token = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.9\n",
        "        )\n",
        "    completion_tokens = token[0][inputs['input_ids'].size(1):]\n",
        "    completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
        "    print('Example ', index)\n",
        "    print(line)\n",
        "    print(f'prompt for one-shot:\\n{line}\\n{prompt}')\n",
        "    print(line)\n",
        "    print(f'human summary:\\n{summary}')\n",
        "    print(line)\n",
        "    print(f'model output for one-shot:\\n{completion}\\n')\n",
        "    print(line)\n",
        "    results.append(rouge.compute(predictions=[completion], references=[summary]))\n",
        "    average_score = {\n",
        "    metric: sum([result[metric].mid.fmeasure for result in results]) / len(results)\n",
        "    for metric in results[0]}\n",
        "    print(average_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn3TlKSW5Aeb"
      },
      "source": [
        "average of rouge for all training dataset by one-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NXCaG-L4_w5",
        "outputId": "7a4e583f-a905-40c6-ba3a-f6874e0e10d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/110 [00:03<06:50,  3.76s/it]\u001b[A\n",
            "  2%|â–         | 2/110 [00:07<06:22,  3.54s/it]\u001b[A\n",
            "  3%|â–Ž         | 3/110 [00:10<05:58,  3.35s/it]\u001b[A\n",
            "  4%|â–Ž         | 4/110 [00:13<05:51,  3.32s/it]\u001b[A\n",
            "  5%|â–         | 5/110 [00:16<05:47,  3.31s/it]\u001b[A\n",
            "  5%|â–Œ         | 6/110 [00:20<05:46,  3.34s/it]\u001b[A\n",
            "  6%|â–‹         | 7/110 [00:22<05:24,  3.15s/it]\u001b[A\n",
            "  7%|â–‹         | 8/110 [00:26<05:35,  3.29s/it]\u001b[A\n",
            "  8%|â–Š         | 9/110 [00:30<05:59,  3.56s/it]\u001b[A\n",
            "  9%|â–‰         | 10/110 [00:34<06:05,  3.65s/it]\u001b[A\n",
            " 10%|â–ˆ         | 11/110 [00:38<05:54,  3.58s/it]\u001b[A\n",
            " 11%|â–ˆ         | 12/110 [00:41<05:45,  3.52s/it]\u001b[A\n",
            " 12%|â–ˆâ–        | 13/110 [00:44<05:21,  3.32s/it]\u001b[A\n",
            " 13%|â–ˆâ–Ž        | 14/110 [00:47<05:25,  3.39s/it]\u001b[A\n",
            " 14%|â–ˆâ–Ž        | 15/110 [00:51<05:21,  3.38s/it]\u001b[A\n",
            " 15%|â–ˆâ–        | 16/110 [00:54<05:08,  3.28s/it]\u001b[A\n",
            " 15%|â–ˆâ–Œ        | 17/110 [00:57<05:01,  3.24s/it]\u001b[A\n",
            " 16%|â–ˆâ–‹        | 18/110 [01:00<04:49,  3.15s/it]\u001b[A\n",
            " 17%|â–ˆâ–‹        | 19/110 [01:03<04:53,  3.23s/it]\u001b[A\n",
            " 18%|â–ˆâ–Š        | 20/110 [01:06<04:45,  3.17s/it]\u001b[A\n",
            " 19%|â–ˆâ–‰        | 21/110 [01:09<04:38,  3.13s/it]\u001b[A\n",
            " 20%|â–ˆâ–ˆ        | 22/110 [01:13<04:56,  3.37s/it]\u001b[A\n",
            " 21%|â–ˆâ–ˆ        | 23/110 [01:17<05:11,  3.58s/it]\u001b[A\n",
            " 22%|â–ˆâ–ˆâ–       | 24/110 [01:20<04:57,  3.46s/it]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–Ž       | 25/110 [01:23<04:42,  3.33s/it]\u001b[A\n",
            " 24%|â–ˆâ–ˆâ–Ž       | 26/110 [01:27<04:40,  3.34s/it]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–       | 27/110 [01:31<04:50,  3.50s/it]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–Œ       | 28/110 [01:35<05:14,  3.83s/it]\u001b[A\n",
            " 26%|â–ˆâ–ˆâ–‹       | 29/110 [01:39<05:10,  3.83s/it]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–‹       | 30/110 [01:42<04:47,  3.60s/it]\u001b[A\n",
            " 28%|â–ˆâ–ˆâ–Š       | 31/110 [01:46<04:51,  3.69s/it]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–‰       | 32/110 [01:49<04:38,  3.58s/it]\u001b[A\n",
            " 30%|â–ˆâ–ˆâ–ˆ       | 33/110 [01:53<04:37,  3.60s/it]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆ       | 34/110 [01:57<04:41,  3.71s/it]\u001b[A\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–      | 35/110 [02:01<04:37,  3.71s/it]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 36/110 [02:04<04:15,  3.46s/it]\u001b[A\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 37/110 [02:07<04:18,  3.55s/it]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–      | 38/110 [02:12<04:28,  3.73s/it]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 39/110 [02:15<04:27,  3.76s/it]\u001b[A\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 40/110 [02:19<04:15,  3.65s/it]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 41/110 [02:22<04:00,  3.49s/it]\u001b[A\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 42/110 [02:25<03:45,  3.31s/it]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 43/110 [02:29<03:54,  3.50s/it]\u001b[A\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 44/110 [02:33<04:04,  3.70s/it]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 45/110 [02:36<03:48,  3.52s/it]\u001b[A\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/110 [02:39<03:39,  3.42s/it]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 47/110 [02:42<03:31,  3.36s/it]\u001b[A\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 48/110 [02:46<03:37,  3.51s/it]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 49/110 [02:50<03:32,  3.49s/it]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 50/110 [02:54<03:46,  3.77s/it]\u001b[A\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 51/110 [03:01<04:40,  4.75s/it]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 52/110 [03:05<04:22,  4.53s/it]\u001b[A\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 53/110 [03:08<03:51,  4.07s/it]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 54/110 [03:11<03:28,  3.73s/it]\u001b[A\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 55/110 [03:15<03:21,  3.67s/it]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 56/110 [03:18<03:11,  3.55s/it]\u001b[A\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 57/110 [03:21<03:04,  3.49s/it]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 58/110 [03:25<03:04,  3.54s/it]\u001b[A\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 59/110 [03:29<03:07,  3.67s/it]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 60/110 [03:32<03:02,  3.64s/it]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 61/110 [03:36<02:50,  3.48s/it]\u001b[A\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 62/110 [03:39<02:41,  3.37s/it]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 63/110 [03:41<02:30,  3.20s/it]\u001b[A\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/110 [03:45<02:32,  3.32s/it]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 65/110 [03:48<02:27,  3.27s/it]\u001b[A\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 66/110 [03:51<02:20,  3.20s/it]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 67/110 [03:54<02:13,  3.11s/it]\u001b[A\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 68/110 [03:58<02:15,  3.23s/it]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 69/110 [04:01<02:15,  3.32s/it]\u001b[A\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 70/110 [04:04<02:10,  3.26s/it]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 71/110 [04:08<02:09,  3.32s/it]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 72/110 [04:11<02:05,  3.29s/it]\u001b[A\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 73/110 [04:15<02:04,  3.36s/it]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 74/110 [04:17<01:56,  3.24s/it]\u001b[A\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 75/110 [04:21<01:52,  3.21s/it]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 76/110 [04:24<01:53,  3.35s/it]\u001b[A\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 77/110 [04:28<01:55,  3.49s/it]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 78/110 [04:31<01:49,  3.43s/it]\u001b[A\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 79/110 [04:35<01:49,  3.53s/it]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 80/110 [04:39<01:45,  3.51s/it]\u001b[A\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 81/110 [04:43<01:45,  3.62s/it]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 82/110 [04:46<01:43,  3.69s/it]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 83/110 [04:50<01:37,  3.62s/it]\u001b[A\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 84/110 [04:54<01:35,  3.69s/it]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 85/110 [04:58<01:39,  3.99s/it]\u001b[A\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 86/110 [05:02<01:36,  4.03s/it]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 87/110 [05:07<01:35,  4.15s/it]\u001b[A\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 88/110 [05:11<01:33,  4.23s/it]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 89/110 [05:14<01:22,  3.91s/it]\u001b[A\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 90/110 [05:18<01:13,  3.67s/it]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 91/110 [05:21<01:10,  3.69s/it]\u001b[A\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 92/110 [05:25<01:07,  3.73s/it]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 93/110 [05:29<01:04,  3.79s/it]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 94/110 [05:32<00:58,  3.64s/it]\u001b[A\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 95/110 [05:36<00:52,  3.53s/it]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 96/110 [05:39<00:50,  3.58s/it]\u001b[A\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 97/110 [05:43<00:48,  3.74s/it]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 98/110 [05:47<00:45,  3.76s/it]\u001b[A\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 99/110 [05:51<00:40,  3.67s/it]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 100/110 [05:55<00:37,  3.79s/it]\u001b[A\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 101/110 [05:59<00:33,  3.77s/it]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 102/110 [06:02<00:28,  3.57s/it]\u001b[A\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 103/110 [06:05<00:23,  3.42s/it]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 104/110 [06:08<00:20,  3.35s/it]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 105/110 [06:12<00:17,  3.45s/it]\u001b[A\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 106/110 [06:15<00:13,  3.30s/it]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 107/110 [06:18<00:09,  3.23s/it]\u001b[A\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 108/110 [06:21<00:06,  3.20s/it]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 109/110 [06:25<00:03,  3.39s/it]\u001b[A\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [06:29<00:00,  3.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'rouge1': 0.3201767403191365, 'rouge2': 0.10116457342954364, 'rougeL': 0.25506544345333143, 'rougeLsum': 0.26571756897339766}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "for index in tqdm(range(110)):\n",
        "\n",
        "    conversation = dataset['test'][index]['conversation']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "    if 109 > index:\n",
        "      prompt = make_prompt([index+1], index)\n",
        "    else:\n",
        "      prompt = make_prompt([30], index)\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "    token = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.9\n",
        "        )\n",
        "    completion_tokens = token[0][inputs['input_ids'].size(1):]\n",
        "    completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
        "    results.append(rouge.compute(predictions=[completion], references=[summary]))\n",
        "average_score = {metric: sum([result[metric].mid.fmeasure for result in results]) / len(results)\n",
        "                  for metric in results[0]}\n",
        "print(average_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpOk6k3PKFoK"
      },
      "source": [
        "#part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Uoa8DRVcK38",
        "outputId": "3e9857bc-776a-4b31-cd4e-10f04b300c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is a conversation between a human and an AI agent. With a summary of the conversation.\n",
            "### Input:\n",
            "Customer: On went to buy a Lip na mpesa wrist band in your shop in Thika and was sent to Annas Mall where i dint find your Team!\n",
            "Agent: Apologies for this, check with any M-PESA Agent, or Safaricom authorized dealer. You may also go back to our shop in Thika -(contd)to verify whether we have restocked.\n",
            "Customer: I was at your shop at Thika and they told me they were not sure where the guys dealing with the wrist bands were\n",
            "Agent: The mpesa 1 tap is not available at Thika its being piloted in Nairobi, Mombasa, Kisumu, Eldoret and Nyeri towns ... cont... currently. To get it you may visit the nearest town which is Nairobi to purchase one at a cost of 20/-.\n",
            "Customer: Would you please confirm for me if I will get the wrist band at your shop in Moi Avenue today so I don't have to travel for nothing.\n",
            "Agent: Hi, yes you will and at the moment it can only be used in the towns mentioned in our previous response only.\n",
            "Customer: On Tuesday I went to your shop on Moi Avenue to get wrist band but was told they ended,but was told I would be informed when they are ready Cont...Are they still not ready yet? ?\n",
            "Agent: Hi, you can visit any authorized dealer or the other shops in town for one.\n",
            "\n",
            "### Output:\n",
            "Customer wants the wrist band which was not available in the stop so customer says to confirm if the wrist band is present. Agent says to visit any authorized dealer or shop in town for one.\n",
            "\n",
            "Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
            "### Input:\n",
            "Customer: if you have a commercial that says you have 100mbs for 44.99 honor it in all your markets you provide 100mbs at.\n",
            "Agent: Good morning. In some areas of our markets our network is not yet able to provide those speeds. We do not offer a...\n",
            "Customer: My area offered me 100mb for more money so now that this is the standard and they do offer 100mb shouldnâ€™t I receive it right away\n",
            "Agent: I would be happy to take a look. If you upgraded to the 100 plan and are not receiving this we can investigate. H...\n",
            "Customer: I have the 60mb plan but I watch many commercials saying they offer 100mb for the same price. My area has 100mb so shouldnâ€™t I be getting it\n",
            "Agent: I am sorry, the advertisements are package/product prices for new customers. If you would like to upgrade that sh...\n",
            "Customer: So you leave existing customers below your new standards is what your telling me, right?\n",
            "Agent: Hi William, In order to discuss this further we ask that you please contact our billing specialist at: 800-892-4357â€‹â€‹. Thank you.\n",
            "\n",
            "### Output:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def make_prompt(examples, sample):\n",
        "    prompt = ''\n",
        "    for index in examples:\n",
        "        conversation = dataset['test'][index]['conversation']\n",
        "        summary = dataset['test'][index]['summary']\n",
        "        prompt += f\"\"\"Below is a conversation between a human and an AI agent. With a summary of the conversation.\n",
        "### Input:\n",
        "{conversation}\n",
        "### Output:\n",
        "{summary}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    conversation = dataset['test'][sample]['conversation']\n",
        "\n",
        "    prompt += f\"\"\"Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
        "### Input:\n",
        "{conversation}\n",
        "### Output:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "prompt = print(make_prompt([30], 109))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JtZQi8DrpI8O"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model, TaskType\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78s5dY9GJogP",
        "outputId": "3d6208c6-b8f8-4940-9e5f-54f9a0c27a33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-3b-4e1t\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"stabilityai/stablelm-3b-4e1t\",\n",
        "    use_safetensors=True,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJDoun0KRXRY"
      },
      "source": [
        "rank 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4O9fCWbErkK",
        "outputId": "a6f3ef5e-b32c-48cd-a6d9-f9851ee2b9a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 10,485,760 || all params: 2,805,928,960 || trainable%: 0.3737001238976485\n"
          ]
        }
      ],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\"\n",
        "    ],\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "VORx3_dYF4HU",
        "outputId": "08a8bf65-8d2e-4693-8be1-db5607691f49"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4395' max='4395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4395/4395 33:41, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.682000</td>\n",
              "      <td>2.075531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.937400</td>\n",
              "      <td>2.058391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.305800</td>\n",
              "      <td>2.063044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.901800</td>\n",
              "      <td>2.086608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.673800</td>\n",
              "      <td>2.099597</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory lora-r16-finetuned/checkpoint-879 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory lora-r16-finetuned/checkpoint-1758 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory lora-r16-finetuned/checkpoint-2637 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory lora-r16-finetuned/checkpoint-3516 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4395, training_loss=1.929868298626597, metrics={'train_runtime': 2024.224, 'train_samples_per_second': 2.171, 'train_steps_per_second': 2.171, 'total_flos': 2.27875588990464e+16, 'train_loss': 1.929868298626597, 'epoch': 5.0})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"lora-r16-finetuned\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=5,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length= 4096,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_1rZ2GdqOpbY"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  \"stabilityai/stablelm-3b-4e1t\",\n",
        "  trust_remote_code=True,\n",
        "  torch_dtype=\"auto\",\n",
        "  use_safetensors=True,\n",
        ").to(\"cuda\")\n",
        "peft_model = PeftModel.from_pretrained(model, \"lora-r16-finetuned/checkpoint-4395\", from_transformers=True)\n",
        "model = peft_model.merge_and_unload()\n",
        "model.config.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_B56c_kt5Lc",
        "outputId": "530eb139-7aa8-4731-cab1-ba54e6cbecfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [13:29<00:00,  7.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average score one shot:{'rouge1': 0.34503884108694416, 'rouge2': 0.12154916425886186, 'rougeL': 0.26681057051662654, 'rougeLsum': 0.2816826578251633}\n",
            "average score zero shot:{'rouge1': 0.4091589271547215, 'rouge2': 0.23245091326230136, 'rougeL': 0.3212717036658451, 'rougeLsum': 0.34940292196154804}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results_zero_shot = []\n",
        "results_one_shot = []\n",
        "for index in tqdm(range(110)):\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "    summary = dataset['test'][index]['summary']\n",
        "    if 109 > index:\n",
        "      prompt = make_prompt([index+1], index)\n",
        "    else:\n",
        "      prompt = make_prompt([30], index)\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "    token = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        temperature=0.75,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        )\n",
        "    completion_tokens = token[0][inputs['input_ids'].size(1):]\n",
        "    completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
        "    results_one_shot.append(rouge.compute(predictions=[completion], references=[summary]))\n",
        "    prompt = dataset['test'][index]['text']\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "    token = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64,\n",
        "        temperature=0.75,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        )\n",
        "    completion_tokens = token[0][inputs['input_ids'].size(1):]\n",
        "    completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
        "    results_zero_shot.append(rouge.compute(predictions=[completion], references=[summary]))\n",
        "average_score_zero_shot = {metric: sum([result[metric].mid.fmeasure for result in results_zero_shot]) / len(results_zero_shot)\n",
        "                  for metric in results_zero_shot[0]}\n",
        "average_score_one_shot = {metric: sum([result[metric].mid.fmeasure for result in results_one_shot]) / len(results_one_shot)\n",
        "                  for metric in results_one_shot[0]}\n",
        "print(f\"average score one shot:{average_score_one_shot}\\naverage score zero shot:{average_score_zero_shot}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPd6W2rvoAZM",
        "outputId": "e1e6337f-f2fc-456e-a725-2e986287347b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): StableLMEpochForCausalLM(\n",
              "      (model): StableLMEpochModel(\n",
              "        (embed_tokens): Embedding(50304, 2560)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x DecoderLayer(\n",
              "            (self_attn): Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2560, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2560, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MLP(\n",
              "              (gate_proj): Linear4bit(in_features=2560, out_features=6912, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=2560, out_features=6912, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=6912, out_features=2560, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "            (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2560, out_features=50304, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ02oLa8oCHK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
